import { Callout, Steps, Step } from "nextra-theme-docs";

# Non-Agentic Workflows

In a traditional non-agentic AI workflow, the user provides a prompt to the language model, and the model generates a single, static response. This is akin to asking a person to write an essay on a topic and simply having them type the entire essay from start to finish without any iterative revisions or adjustments.

Despite the apparent simplicity of this approach, large language models (LLMs) like GPT-3.5 and GPT-4 are remarkably capable at producing coherent and relevant responses to prompts in this manner. However, this non-agentic workflow can often fall short in delivering the best possible results, as it lacks the iterative, self-reflective process that humans naturally employ when tackling complex tasks.

<Callout type="info">
**Example:** Imagine asking a language model to write code to solve a specific coding problem. In a non-agentic workflow, the model would simply generate a code snippet and return it, without any opportunity to test, debug, or refine the solution.
</Callout>

Contrast this with an **agentic workflow**, where the AI agent is prompted to break down the task, gather relevant information, draft a solution, and then critically evaluate and revise its work. This iterative process can lead to significantly better outcomes, as the agent is able to identify and correct errors, explore alternative approaches, and ultimately deliver a more robust and effective solution.

In the next section, we'll dive deeper into the key design patterns that enable these powerful agentic AI workflows.